{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projet YOLO Gestures\n",
    "BURDAIRON Florian BLUMET Thomas 5A Polytech Lyon (12/2024)\n",
    "## Description\n",
    "L'objectif a été de réutiliser le modèle de réseau de neurones YOLO (de type CNN) qui permet de faire de la classification et de la détection d'objets. Le modèle (en version 8) a été importé préentraîner. Dans notre objectif de réaliser de la détection de gestes, notamment ceux du jeu Pierre-Papier-Ciseaux, nous avons fine-tuné le modèle en le réentraînant (avec différentes valeurs d'epochs notamment, et une valeur de batch fixé à 8).\n",
    "Pour ce faire, nous avons utilisé le site Roboflow qui permet d'importer des datasets d'images déjà détourées et annotées. Dans notre cas, nous avons importé un dataset existant d'images associé au jeu (cf https://universe.roboflow.com/roboflow-58fyf/rock-paper-scissors-sxsw/dataset/11). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperçu visuel\n",
    "\n",
    "### Vidéo de démonstration\n",
    "\n",
    "[![vidéo démo](https://img.youtube.com/vi/ReloVy038hk/0.jpg)](https://www.youtube.com/embed/ReloVy038hk?si=sfJW1PBMoYLW4kXn)\n",
    "\n",
    "### Exemple classification\n",
    "\n",
    "| Gesture                       | Image                                                                         |\n",
    "|-------------------------------|-------------------------------------------------------------------------------|\n",
    "| Paper                         | <img src=\"img/paper_detection.png\" alt=\"paper_detection\" width=\"500px\">       |\n",
    "| Rock                          | <img src=\"img/rock_detection.png\" alt=\"rock_detection\" width=\"500px\">         |\n",
    "| Scissors                      | <img src=\"img/scissors_detection.png\" alt=\"scissors_detection\" width=\"500px\"> |\n",
    "| 2 detections at the same time | <img src=\"img/round.png\" alt=\"round\" width=\"500px\">                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancement du projet\n",
    "\n",
    "> Remarque : \\\n",
    "> Le fichier [`YOLO_webcam.py`](YOLO_webcam.py) contient l'entièreté du code du projet permettant une exécution simplifiée.\n",
    "\n",
    "### Besoin du projet\n",
    "\n",
    "Pour ce projet, nous avons utilisé plusieurs librairies python :\n",
    "- Ultralytics : pour l'utilisation de YOLO\n",
    "- Roboflow : pour l'importation du jeu de données\n",
    "- Torch : pour l'entrainement du modèle (avec CUDA)\n",
    "- OpenCV : pour l'utilisation de la webcam\n",
    "- YAML : pour la lecture du fichier de description du jeu de données\n",
    "\n",
    "> Remarque : \\\n",
    "> Pour réaliser l'entraînement, il faut importer la librairie Torch CUDA et posséder une carte graphique Nvidia. \\\n",
    "> L'utilisation des modèles que nous avons déjà entrainé (`model/`) ne nécessite pas l'utilisation de CUDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from ultralytics import YOLO, settings\n",
    "import cv2\n",
    "import math \n",
    "import torch\n",
    "import os\n",
    "from roboflow import Roboflow\n",
    "import yaml\n",
    "\n",
    "# Select the device to be used\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Select the directory where the datasets are stored\n",
    "settings.update(datasets_dir='.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation du jeu de données et entrainement du modèle\n",
    "\n",
    "Voici une description des fonctions que nous utilisons dans le projet :\n",
    "- `train_and_save_model` : Permet d'entrainer le modèle sur un jeu de données spécifique (`model_name`) et de sauvegarder les points du réseau. Il est possible de choisir des paramètre pour l'entrainement (`nb_epochs` et `batch_size`).\n",
    "- `get_class_names` : Permet de récupérer la liste des éléments classifiables contenue dans le fichier de description du jeu de données `data.yaml`.\n",
    "- `load_model_from_roboflow` : Permet de télécharger le jeu de données directement depuis Roboflow s'il n'est pas déjà dans `data/` puis d'entrainer le modèle si celui-ci n'a as déjà été entrainer dessus.\n",
    "- `load_model` : Permet de récuperer le modèle correspondant a un jeu de données déjà présent dans `data/` et de l'entrainer si il ne l'a pas déjà été. (Non utilisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(model_name,nb_epochs=10,batch_size=8) -> YOLO:\n",
    "    \"\"\"\n",
    "    Train and save model\n",
    "\n",
    "    Model will be saved in `model/` directory\n",
    "    \"\"\"\n",
    "    # Initialize model with pre-trained weights (YOLO v8)\n",
    "    model = YOLO(\"yolo-Weights/yolov8n.pt\")\n",
    "\n",
    "    # Train model on custom dataset\n",
    "    model.train(data=\"data/\" + model_name + \"/data.yaml\", epochs=nb_epochs, batch=batch_size, device=device)\n",
    "\n",
    "    # Save model in `model/` directory\n",
    "    model.save(\"model/\" + model_name + \"_e\" + str(nb_epochs) + \"_b\" + str(batch_size) + \".pt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_class_names(yaml_path):\n",
    "    \"\"\"Get class names from yaml file\"\"\"\n",
    "    with open(yaml_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data['names']\n",
    "\n",
    "\n",
    "def load_model_from_roboflow(workspace, project_name, version_number, model_name,nb_epochs,batch_size) -> tuple[YOLO, any]:\n",
    "    \"\"\"\n",
    "    Load model from Roboflow\n",
    "\n",
    "    If the dataset is not in `data/` directory, then it will be downloaded\n",
    "\n",
    "    If the model is not in `model/` directory, then it will be trained and its weights will be saved\n",
    "    \"\"\"\n",
    "    # Download dataset if not exists\n",
    "    if not os.path.exists(\"data/\" + model_name):\n",
    "        rf = Roboflow(api_key=\"8DrZ8Cjqqu2mLaJM9iPH\")\n",
    "        project = rf.workspace(workspace).project(project_name)\n",
    "        version = project.version(version_number)\n",
    "        dataset = version.download(\"yolov8\", \"data/\" + model_name)\n",
    "    \n",
    "    # Get class names from yaml file\n",
    "    classNames = get_class_names(\"data/\" + model_name + \"/data.yaml\")\n",
    "\n",
    "    # Load model if exists\n",
    "    if os.path.exists(\"model/\" + model_name + \"_e\" + str(nb_epochs) + \"_b\" + str(batch_size) + \".pt\"):\n",
    "        return YOLO(\"model/\" + model_name + \"_e\" + str(nb_epochs) + \"_b\" + str(batch_size) + \".pt\"), classNames\n",
    "    \n",
    "    # If model does not exist, then train and save it\n",
    "    return train_and_save_model(model_name,nb_epochs,batch_size), classNames\n",
    "\n",
    "def load_model(model_name,nb_epochs,batch_size) -> tuple[YOLO, any]:\n",
    "    \"\"\"\n",
    "    Load model\n",
    "    \n",
    "    The dataset should be in `data/` directory\n",
    "\n",
    "    If the model is not in `model/` directory, then it will be trained and its weights will be saved\n",
    "    \"\"\"\n",
    "    # Get class names from yaml file\n",
    "    classNames = get_class_names(\"data/\" + model_name + \"/data.yaml\")\n",
    "\n",
    "    # Load model if exists\n",
    "    if os.path.exists(\"model/\" + model_name + \"_e\" + str(nb_epochs) + \"_b\" + str(batch_size) + \".pt\"):\n",
    "        model = YOLO(\"model/\" + model_name + \"_e\" + str(nb_epochs) + \"_b\" + str(batch_size) + \".pt\")\n",
    "    # If model does not exist, then train and save it\n",
    "    else:\n",
    "        model = train_and_save_model(model_name,nb_epochs,batch_size)\n",
    "\n",
    "    return model, classNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model rock-paper-scissors...\n",
      "New https://pypi.org/project/ultralytics/8.3.49 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo-Weights/yolov8n.pt, data=data/rock-paper-scissors/data.yaml, epochs=5, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train8\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\PolytechTP5AHandGestureGames\\data\\rock-paper-scissors\\train\\labels.cache... 14966 images, 5957 backgrounds, 0 corrupt: 100%|██████████| 14966/14966 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\PolytechTP5AHandGestureGames\\data\\rock-paper-scissors\\valid\\labels.cache... 588 images, 247 backgrounds, 0 corrupt: 100%|██████████| 588/588 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train8\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5       1.2G      1.455       2.74      1.611          8        640: 100%|██████████| 1871/1871 [01:46<00:00, 17.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 37/37 [00:01<00:00, 20.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        588        406      0.581      0.506      0.539      0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5       1.2G      1.427      1.849      1.566          5        640: 100%|██████████| 1871/1871 [01:38<00:00, 19.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 37/37 [00:01<00:00, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        588        406      0.788      0.722      0.815      0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      1.24G      1.369      1.573      1.517         12        640: 100%|██████████| 1871/1871 [01:35<00:00, 19.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 37/37 [00:01<00:00, 20.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        588        406      0.865      0.722      0.817       0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5       1.2G      1.285      1.339      1.445          8        640: 100%|██████████| 1871/1871 [01:34<00:00, 19.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 37/37 [00:01<00:00, 20.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        588        406      0.876       0.83      0.894      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      1.24G      1.195      1.145      1.376          7        640: 100%|██████████| 1871/1871 [01:34<00:00, 19.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 37/37 [00:01<00:00, 21.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        588        406      0.939      0.888      0.943      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.139 hours.\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train8\\weights\\best.pt...\n",
      "Ultralytics 8.3.24  Python-3.12.4 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4080, 16376MiB)\n",
      "Model summary (fused): 168 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 37/37 [00:01<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        588        406      0.939      0.888      0.943      0.558\n",
      "                 Paper        134        141      0.903      0.857      0.929      0.526\n",
      "                  Rock        125        147       0.94      0.925      0.945      0.544\n",
      "              Scissors        114        118      0.974      0.881      0.954      0.604\n",
      "Speed: 0.1ms preprocess, 0.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train8\u001b[0m\n",
      "Model loaded successfully\n",
      "Class names:  ['Paper', 'Rock', 'Scissors']\n"
     ]
    }
   ],
   "source": [
    "# For the example, load the dataset from Roboflow and train the model with 5 epochs and batch size of 8\n",
    "def example() -> tuple[YOLO, any]:\n",
    "    \"\"\"\n",
    "    Get model\n",
    "\n",
    "    If the dataset is not in `data/` directory, then it will be downloaded\n",
    "\n",
    "    If the model is not in `model/` directory, then it will be trained and its weights will be saved\n",
    "    \"\"\"\n",
    "    model_name = \"rock-paper-scissors\"\n",
    "    print(\"Loading model \" + model_name + \"...\")\n",
    "\n",
    "    # Can replace the following line with other parameters to match the trained model from `model/` directory to avoid the trainig process\n",
    "    return load_model_from_roboflow(\"roboflow-58fyf\", \"rock-paper-scissors-sxsw\", 11, model_name, 5, 8) # workspace, project_name, version_number, model_name, nb_epochs, batch_size\n",
    "    # return load_model_from_roboflow(\"roboflow-58fyf\", \"rock-paper-scissors-sxsw\", 11, model_name, 50, 8) # Best model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear cuda cache before running the code\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    model, classNames = example()\n",
    "    print(\"Model loaded successfully\")\n",
    "    print(\"Class names: \", classNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancement du jeu\n",
    "\n",
    "Quand le jeu démarre, une fenêtre apparaît avec la webcam et une interface séparant la fenêtre en deux. Chaque côté correspond à un joueur et affiche son score et le geste qu'il a choisi durant la manche.\n",
    "\n",
    "Une manche dure 50 frames. Cette durée est représentée par la barre de progression en bas de la fenêtre. À la fin de chaque manche, un vainqueur est désigné de la manière suivante :\n",
    "- Si un des 2 joueurs n'a fait aucun geste, aucun vainqueur n'est désigné.\n",
    "- Si les deux joueurs ont fait le même geste, aucun vainqueur n'est désigné.\n",
    "- Sinon les règles classiques du **Pierre Papier Ciseaux** s'appliquent (Pierre bat Ciseaux, Ciseaux bat Papier et Papier bat Pierre).\n",
    "\n",
    "Pour faire un geste le joueur doit faire un geste dans sa partie de l'écran. Il a ensuite une marge de manoeuvre de 20 frames pour changer son geste (cela sert à éviter qu'une détection soit réalisé avant que le joueur finisse de faire son geste).\n",
    "\n",
    "Nous avons choisi de faire des manches assez courtes pour éviter la triche (un joueur attend que l'autre joue pour connaître son geste avant de jouer). Le fait de fixer le choix d'un joueur permet d'éviter qu'il change de geste pendant la manche (de manière volontaire pour tricher ou de manière involontaire dûe à une erreur de classification).\n",
    "\n",
    "> Pour quitter le jeu, il faut appuyer sur la touche `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 33.6ms\n",
      "Speed: 1.5ms preprocess, 33.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.0ms preprocess, 4.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 0.7ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 0.5ms preprocess, 4.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.5ms preprocess, 3.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 0.0ms preprocess, 5.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.1ms preprocess, 5.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Rock, 4.2ms\n",
      "Confidence ---> 0.28\n",
      "Class name --> Rock\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.5ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.0ms preprocess, 4.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.4ms preprocess, 3.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 0.5ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.0ms preprocess, 4.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 1.6ms preprocess, 4.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.5ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 2.1ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 1.5ms preprocess, 4.4ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.8ms\n",
      "Speed: 1.7ms preprocess, 3.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 0.5ms preprocess, 4.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 0.0ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.0ms preprocess, 4.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 0.0ms preprocess, 8.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 0.5ms preprocess, 6.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.0ms preprocess, 3.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.0ms preprocess, 4.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.0ms preprocess, 5.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.1ms preprocess, 6.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.0ms preprocess, 4.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.5ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 0.5ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 0.6ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 2.0ms preprocess, 4.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 0.5ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.0ms preprocess, 4.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Rock, 4.6ms\n",
      "Confidence ---> 0.3\n",
      "Class name --> Rock\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Rock, 5.2ms\n",
      "Confidence ---> 0.33\n",
      "Class name --> Rock\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.0ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 2.1ms preprocess, 4.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.0ms preprocess, 5.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.0ms preprocess, 5.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.0ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 0.5ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 0.5ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.0ms preprocess, 6.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.5ms preprocess, 4.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.6ms\n",
      "Speed: 1.7ms preprocess, 3.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.0ms preprocess, 4.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 0.0ms preprocess, 5.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.0ms\n",
      "Speed: 1.5ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "# Initialize game variables\n",
    "score_player1 = 0\n",
    "score_player2 = 0\n",
    "sign_player1 = \"\"\n",
    "sign_player2 = \"\"\n",
    "player1_first_detect = 100\n",
    "player2_first_detect = 100\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "# Start game loop\n",
    "while cap.isOpened():\n",
    "    # Capture a frame\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to capture image\")\n",
    "        continue\n",
    "\n",
    "    # Mirror the image horizontally\n",
    "    img = cv2.flip(img, 1)\n",
    "    \n",
    "    frame_count += 1\n",
    "\n",
    "    # Detect objects in the image using YOLO\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # Build UI with scores and signs\n",
    "    cv2.rectangle(img, (0, 0), (637, 720), (0, 0, 255), 3)\n",
    "    cv2.rectangle(img, (642, 0), (1280, 720), (255, 0, 0), 3)\n",
    "    cv2.putText(img, \"Player 1: \" + str(score_player1), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.putText(img, \"Player 2: \" + str(score_player2), (652, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.putText(img, sign_player1, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.putText(img, sign_player2, (652, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "    # Display progress bar for 50 frames to show the time limit for the round\n",
    "    cv2.rectangle(img, (0, 700), (round(frame_count / 50 * 1280), 720), (0, 255, 0), -1)\n",
    "\n",
    "    # Loop through the results and draw bounding boxes\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "\n",
    "        for box in boxes:\n",
    "            # Get bounding box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "\n",
    "            # Get confidence\n",
    "            confidence = math.ceil((box.conf[0]*100))/100\n",
    "            print(\"Confidence --->\",confidence)\n",
    "\n",
    "            # Get class name\n",
    "            cls = int(box.cls[0])\n",
    "            print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            # Get the mean of x coordinates\n",
    "            x_mean = (x1 + x2) / 2\n",
    "\n",
    "            # Check if the object is on the left or right side of the screen\n",
    "            if x_mean < 640:\n",
    "                # Player 1\n",
    "                # Check if the sign is detected for the first time or not\n",
    "                if sign_player1 == \"\":\n",
    "                    player1_first_detect = frame_count\n",
    "                    sign_player1 = classNames[cls].lower()\n",
    "                # Check if the sign is detected again within 20 frames\n",
    "                elif frame_count - player1_first_detect < 20:\n",
    "                    sign_player1 = classNames[cls].lower()\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                # Player 2\n",
    "                # Check if the sign is detected for the first time or not\n",
    "                if sign_player2 == \"\":\n",
    "                    player2_first_detect = frame_count\n",
    "                    sign_player2 = classNames[cls].lower()\n",
    "                # Check if the sign is detected again within 20 frames\n",
    "                elif frame_count - player2_first_detect < 20:\n",
    "                    sign_player2 = classNames[cls].lower()\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Put box in cam\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "            # Put object details in cam\n",
    "            org = [x1, y1]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            color = (255, 0, 0)\n",
    "            thickness = 2\n",
    "            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
    "\n",
    "    # Check if the round is over\n",
    "    if(frame_count >= 50):\n",
    "        # Check who wins the round and update the scores\n",
    "        if sign_player1 == \"rock\" and sign_player2 == \"scissors\":\n",
    "            score_player1 += 1\n",
    "        elif sign_player1 == \"scissors\" and sign_player2 == \"rock\":\n",
    "            score_player2 += 1\n",
    "        elif sign_player1 == \"scissors\" and sign_player2 == \"paper\":\n",
    "            score_player1 += 1\n",
    "        elif sign_player1 == \"paper\" and sign_player2 == \"scissors\":\n",
    "            score_player2 += 1\n",
    "        elif sign_player1 == \"rock\" and sign_player2 == \"paper\":\n",
    "            score_player2 += 1\n",
    "        elif sign_player1 == \"paper\" and sign_player2 == \"rock\":\n",
    "            score_player1 += 1\n",
    "\n",
    "        # Reset the signs and frame count for the next round\n",
    "        sign_player1 = \"\"\n",
    "        sign_player2 = \"\"\n",
    "        player1_first_detect = 100\n",
    "        player2_first_detect = 100\n",
    "        frame_count = 0\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('Rock Paper Scissors Game', img)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
